{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:31:00.245773Z",
     "start_time": "2025-11-15T21:31:00.244159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "id": "97b9d425e4f92f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install symspellpy"
   ],
   "id": "32a811d385e01a79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This version should chunk into sections + save and resume feature (in case of crash, haven't really tested that here though)\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, LogitsProcessor\n",
    "import os\n",
    "import string\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Memory cleanup before loading\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Path to model folder - make sure to download this first\n",
    "model_dir = \"/content/drive/MyDrive/my-model-folder\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "print(\"Tokenizer successfully loaded.\")\n",
    "\n",
    "# Load quantized model safely (avoiding meta tensors)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device.upper()}.\")\n",
    "\n",
    "# Explicit device map to control offloading\n",
    "device_map = \"auto\"\n",
    "offload_folder = \"/content/offload\"  # temporary folder for swapped layers\n",
    "os.makedirs(offload_folder, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=False,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    local_files_only=True,\n",
    "    offload_folder=offload_folder\n",
    ")\n",
    "\n",
    "# Force materialization (no meta tensors)\n",
    "for _, p in model.named_parameters():\n",
    "    if p.device.type == \"meta\":\n",
    "        p.data = torch.zeros_like(p, device=\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Token masking setup\n",
    "# Remove non-English letters and most punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + \"!\\\"$\\',.;?\" + \" \\n\"\n",
    "allowed_tokens = []\n",
    "\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    token_str = tokenizer.decode([i])\n",
    "    if all(ch in allowed_chars for ch in token_str):\n",
    "        allowed_tokens.append(i)\n",
    "\n",
    "disallowed_tokens = [\n",
    "    i for i in range(tokenizer.vocab_size)\n",
    "    if i not in allowed_tokens and i not in tokenizer.all_special_ids\n",
    "]\n",
    "\n",
    "# Letters to omit\n",
    "# omit_list = \"zqjxkvbpgyfmw\" # least frequent in texts\n",
    "# omit_list = \"qjxzwkvfybhmp\" # least frequent in dictionaries\n",
    "# omit_list = \"zqxjkvcdugmpy\" # chat gpt recommendation\n",
    "omit_list = \"e\" # letters to mask/ eliminate from LLM output\n",
    "special_tokens = tokenizer.all_special_ids\n",
    "n_tokens = tokenizer.vocab_size\n",
    "\n",
    "def detect_unwanted_letters(token_str):\n",
    "    omit_letters = \"\".join([o.upper() + o.lower() for o in omit_list])\n",
    "    return any(c in omit_letters for c in token_str)\n",
    "\n",
    "mask_indices = [\n",
    "    i for i in range(n_tokens)\n",
    "    if detect_unwanted_letters(tokenizer.decode([i])) and i not in special_tokens\n",
    "]\n",
    "\n",
    "# all characters to mask\n",
    "final_mask = list(set(mask_indices) | set(disallowed_tokens))\n",
    "\n",
    "print(f\"Model  total number of tokens: {n_tokens:,}\")\n",
    "print(f\"Masked total number of tokens: {len(final_mask):,}\")\n",
    "\n",
    "class MaskLogits(LogitsProcessor):\n",
    "    def __init__(self, mask_indices):\n",
    "        self.mask_indices = mask_indices\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores[:, self.mask_indices] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "# spell checker (sometimes model will output words that don't exist - some of them can be corrected without adding omitted letters)\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(\"/content/drive/MyDrive/word_dictionary.txt\", term_index=0, count_index=1)\n",
    "\n",
    "def clean_text(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        if not word.isalpha():\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "        suggestions = sym_spell.lookup(word.lower(), Verbosity.CLOSEST)\n",
    "        if suggestions and not any(ch in omit_list for ch in suggestions[0].term):\n",
    "            term = suggestions[0].term\n",
    "            new_word = term.capitalize() if word[0].isupper() else term\n",
    "            corrected_words.append(new_word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "# Path to saved file (i.e. file in which novel is saved)\n",
    "existing_file_prompt = input(\"Enter path to existing novel file (hit ENTER if new novel): \")\n",
    "\n",
    "def get_existing_novel(file_path):\n",
    "  parts = {\"current-chunk\": \"\"}\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    sections = content.split(\"\\n\\n\")\n",
    "\n",
    "  chunk = 0\n",
    "  for section in sections:\n",
    "    if section.startswith(\"Prompt: \"):\n",
    "      parts[\"prompt\"] = section\n",
    "    elif section.startswith(\"Characters: \"):\n",
    "      parts[\"characters\"] = section\n",
    "    elif section.startswith(\"Setting: \"):\n",
    "      parts[\"setting\"] = section\n",
    "    elif section.startswith(\"Tone: \"):\n",
    "      parts[\"tone\"] = section\n",
    "    elif section.startswith(\"Chapters: \"):\n",
    "      parts[\"chapters\"] = section\n",
    "    elif not section.startswith(\"CHAPTER \"): # the actual text, not including the chapter title\n",
    "      parts[\"prev-chunk\"] = parts[\"current-chunk\"]\n",
    "      parts[\"current-chunk\"] = section[-1000:] # will include chapter highlight as well\n",
    "      parts[\"chunk\"] = chunk\n",
    "      chunk += 1\n",
    "\n",
    "    print(f\"Section Retrieved: {section}\")\n",
    "\n",
    "  return parts\n",
    "\n",
    "# Gets existing novel pieces (if they exist/ resuming story)\n",
    "existing_novel_parts = get_existing_novel(existing_file_prompt) if existing_file_prompt else None\n",
    "\n",
    "# Text generation\n",
    "print(\"Model is ready with runtime token masking.\")\n",
    "prompt = existing_novel_parts[\"prompt\"] if existing_file_prompt else input(\"> \")\n",
    "\n",
    "# Save prompt\n",
    "file_path = existing_file_prompt if existing_file_prompt else f\"/content/drive/MyDrive/novel_{prompt}.txt\"\n",
    "if not existing_file_prompt:\n",
    "  with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Prompt: \" + prompt + \"\\n\\n\")\n",
    "\n",
    "# Generation\n",
    "def generation(gen_prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": gen_prompt}]\n",
    "    chat_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,  # ensures model knows it’s time to reply\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            chat_input,\n",
    "            max_new_tokens=5000,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2, # penalize repeated tokens\n",
    "            no_repeat_ngram_size=4, # prevents same 4-token phrase repetition\n",
    "            logits_processor=[MaskLogits(final_mask)]\n",
    "        )\n",
    "\n",
    "    # Decode and print response (just the new part, not the prompt)\n",
    "    og_response = tokenizer.decode(\n",
    "        output_ids[0][chat_input.shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    return clean_text(og_response)\n",
    "\n",
    "starter_prompt = f\"I want to write a full-length novel based on the following idea: {prompt}\"\n",
    "\n",
    "# used for enforcing who the characters are, setting, and tone\n",
    "def generate_story_background():\n",
    "  if existing_file_prompt:\n",
    "    return existing_novel_parts[\"characters\"], existing_novel_parts[\"setting\"], existing_novel_parts[\"tone\"]\n",
    "\n",
    "  character_length = \"This generation should be no longer than 200 words.\"\n",
    "  setting_length = \"This generation should contain no more than 40 words.\"\n",
    "  tone_length = \"This generation should be no longer than 80 words.\"\n",
    "\n",
    "  character_prompt = f\"{starter_prompt} Create a list of characters that fit this story, including demographic information, personality traits, and how they physically look.\"\n",
    "  character_add = \"Every generated sentence should describe exactly one character.  Each character should only have one sentence describing them.  There should be an equal number of sentences as there are characters. Each sentence should list the character\\’s name, two to three pieces of demographic information, and two to three pieces of information that describe their personality.  The character should be described in this order and there should be no additional filler or unnecessary numbering of the demographic or personality traits. Do not make them optional choices to choose from.  Instead, each character must have a specific role that makes them integral to the story. An individual character description should take up no more than 35 words.\"\n",
    "  setting_prompt = f\"{starter_prompt} Create the general setting for this story, including the year (or range of years) and location.\"\n",
    "  setting_add = \"The generated output should contain exactly two sentences.  The first sentence should describe the time period in which the story takes place.  The second sentence should describe the general location or locations that the story takes place.  For example, if the story is about a man traveling to every country, the setting will be the world. Do not add any filler context. Do not give a list of possible settings.  Only generate one setting that the story takes place in.\"\n",
    "  tone_prompt = f\"{starter_prompt} Generate the tone that this story should follow, which should dictate the emotional and intellectual stance of the story.\"\n",
    "  tone_add = \"Directly answer this prompt by outlining the type of word choice, sentence structure, and style of writing. Do not add any filler context.  Do not generate different options for the tone.  Only generate one style of tone.\"\n",
    "\n",
    "  characters = generation(f\"{character_prompt} {character_add} {character_length}\")\n",
    "  print(f\"\\nCharacters: {characters}\\n\")\n",
    "  setting = generation(f\"{setting_prompt} {setting_add} Here are a list of characters in this story. Do not reference them in the generation: {characters} {setting_length}\")\n",
    "  print(f\"\\nSetting: {setting}\\n\")\n",
    "  tone = generation(f\"{tone_prompt} {tone_add} Here are a list of characters and the setting in this story. Do not reference them in the generation: Characters: {characters} Setting: {setting} {tone_length}\")\n",
    "  print(f\"\\nTone: {tone}\\n\")\n",
    "\n",
    "  with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Characters: \" + characters + \"\\n\\n\")\n",
    "    f.write(\"Setting: \" + setting + \"\\n\\n\")\n",
    "    f.write(\"Tone: \" + tone + \"\\n\\n\")\n",
    "\n",
    "  return characters, setting, tone\n",
    "\n",
    "# Chunk into pieces - roughly 91,000 words total (~6,500 words generated per 10,000 tokens)\n",
    "num_story_pieces = 14\n",
    "tokens_per_chunk = 10000\n",
    "# Generates the chapters\n",
    "def chunk_text(background_prompt):\n",
    "  if existing_file_prompt:\n",
    "    return existing_novel_parts[\"chapters\"]\n",
    "\n",
    "  general_prompt = f\"{starter_prompt} Create a list of {num_story_pieces} one-sentence story beats that together outline the entire novel from beginning to end. Each sentence should be broad enough to represent about 6,000–7,000 words of story, meaning each one covers many scenes or major developments, not just a single moment. The {num_story_pieces} sentences should form a complete, coherent story arc (setup, rising action, climax, resolution), introduce and evolve characters, conflicts, and themes logically, maintain consistent tone and style, and flow naturally from one sentence to the next. Think of it as a condensed version of the entire novel, told as {tokens_per_chunk} long, detailed chapter summaries — one sentence per chapter.  Each chapter should consist of exactly one sentence and each sentence should use exactly one period. Do not number, label, or list any sentence. Separate each chapter with exactly one period and one space. There should be no other periods anywhere in the text.\"\n",
    "  all_chapters = generation(f\"{general_prompt} {background_prompt}\")\n",
    "  print(f\"\\nAll Chapters: {all_chapters}\\n\")\n",
    "\n",
    "  with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Chapters: \" + all_chapters + \"\\n\\n\")\n",
    "\n",
    "  return all_chapters\n",
    "\n",
    "def chunk_generation(prev_chunk, chapter_number, chapter_sentence, background_prompt, prior_story=''):\n",
    "  # Generate chapter\n",
    "  with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"CHAPTER {chapter_number}: {chapter_sentence}\\n\\n\")\n",
    "\n",
    "  # Generate text\n",
    "  chunk_whole_text = prior_story\n",
    "  constant_text = \"Continue this story exactly where it left off. Do not summarize or restart. Avoid repeating earlier parts of the story. Focus only on new developments.\"\n",
    "  chapter_text = f\"Base the story on the following sentence: {chapter_sentence}\"\n",
    "  additional_prompting = \"Make sure to keep point of view (first, second, or third) consistent throughout the whole generation. Keep characters consistent with any existing story that you are given. Do not add new characters unless absolutely necessary. Make sure linking verbs (such as is and are) are used when needed.\"\n",
    "  initial_prompt = True\n",
    "  continuation_text = f\"Here is the first part of the story: {prev_chunk}\" if prev_chunk else \"\" # text from previous chunk\n",
    "  english_prompt = \"Ensure the story continues naturally and does not repeat the same phrase or description. Avoid using invented or non-existent words (i.e. words that rarely appear in training data). If no suitable synonym exists, rephrase the thought naturally using other words.  Do not use too many symbol characters in a generation.  Only use English letters, words, and punctuation. Even though some letters are missing, ensure your sentences remain grammatical. Always include linking verbs like is, was, are, and were where appropriate and if possible. Write clearly and naturally, even if word choice feels limited.\"\n",
    "  present_tense = \"All text should be in the present tense, if possible.\" # might remove later\n",
    "\n",
    "  while len(tokenizer(chunk_whole_text).input_ids) < tokens_per_chunk:\n",
    "      # Format as a chat-style input (for instruct-tuned models)\n",
    "      if initial_prompt:\n",
    "        full_prompt = f\"{prompt} {chapter_text} {continuation_text[-1000:]} {constant_text}\"\n",
    "      else:\n",
    "        full_prompt = f\"{prompt} Here is the first part of the story: {chunk_whole_text[-1000:]} {constant_text}\" # takes last 1,000 tokens - might want to change logic for more accuracy\n",
    "      full_prompt += (\" \" + english_prompt)\n",
    "      full_prompt += (\" \" + additional_prompting)\n",
    "      full_prompt += (\" \" + background_prompt)\n",
    "      full_prompt += (\" \" + present_tense)\n",
    "\n",
    "      response = generation(full_prompt)\n",
    "      print(f\"Current Response: \\n{response}\\n\")\n",
    "\n",
    "      # Open file in append mode and write\n",
    "      with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\t\" + response + \"\\n\")\n",
    "\n",
    "      # Update variables\n",
    "      initial_prompt = False\n",
    "      chunk_whole_text += (\"\\t\" + response + \"\\n\")\n",
    "\n",
    "      torch.cuda.empty_cache()\n",
    "      gc.collect()\n",
    "\n",
    "  with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\")\n",
    "  return chunk_whole_text\n",
    "\n",
    "def generate_novel():\n",
    "  whole_text = \"\"\n",
    "  current_chunk = existing_novel_parts[\"chunk\"] if existing_file_prompt else 0\n",
    "\n",
    "  background_tuple = generate_story_background()\n",
    "  characters = background_tuple[0]\n",
    "  setting = background_tuple[1]\n",
    "  tone = background_tuple[2]\n",
    "  background_prompt = f\"Make sure to use the following characters: {characters} Make sure to use the following setting: {setting} Make sure to use the following tone: {tone}\"\n",
    "\n",
    "  parts = chunk_text(background_prompt).split(\".\")\n",
    "  previous_text_chunk = existing_novel_parts[\"prev-chunk\"] if existing_file_prompt else \"\"\n",
    "  current_text_chunk = existing_novel_parts[\"current-chunk\"] if existing_file_prompt else \"\"\n",
    "\n",
    "  # This condition is if we are reloading the existing novel; we are finishing off the chapter\n",
    "  if existing_file_prompt:\n",
    "    regen_text = current_text_chunk\n",
    "    current_text_chunk = chunk_generation(previous_text_chunk, current_chunk + 1, parts[current_chunk], background_prompt, regen_text)\n",
    "\n",
    "    # Make sure not to include bit of that chapter that was already added to the text file\n",
    "    whole_text += current_text_chunk[len(regen_text):]\n",
    "    current_chunk += 1\n",
    "\n",
    "  while current_chunk < num_story_pieces:\n",
    "    previous_text_chunk = current_text_chunk\n",
    "\n",
    "    current_text_chunk = chunk_generation(previous_text_chunk, current_chunk + 1, parts[current_chunk], background_prompt)\n",
    "    whole_text += current_text_chunk\n",
    "    current_chunk += 1\n",
    "\n",
    "    print(f\"Chunk {current_chunk} complete!\")\n",
    "\n",
    "  print(\"Whole Generation: \" + whole_text)\n",
    "\n",
    "generate_novel()"
   ],
   "id": "f1de876dcd5422c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
